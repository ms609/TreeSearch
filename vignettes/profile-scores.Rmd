---
title: "Profile parsimony"
author: "Martin R. Smith"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
csl: ../inst/apa-old-doi-prefix.csl
vignette: >
  %\VignetteIndexEntry{Profile parsimony}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document aims to give a flavour of the justification for using parsimony
in general, and profile parsimony in particular, to reconstruct evolutionary
history; and to summarize the mathematical/information theoretic underpinning
of the profile parsimony approach.


# Profile parsimony: philosophy

There are some a number of complementary perspectives on the philosophical
justificaion for a parsimony, which this brief overview will not do justice to;
but hopefully my unqualified and unreferenced perspective captures some of the
nature of the principal arguments.
Better versed readers are invited to suggest improvements or modifications
by e-mail or by opening a [GitHub issue](https://github.com/ms609/TreeSearch/issues/new/).

Parsimony has been defended by reference to Occam's Razor: the principle that
science should prefer the simplest explanation that satisfactorily accounts
for observations.  In the context of morphological phylogenetics, "observations"
are codified as scorings of observed character states in a matrix.
Ideally, a phylogenetic tree would explain the distribution of character states
between taxa by reconstructing each character state as representing a 
homologous feature, with a single evolutionary origin on the tree.
A scenario in which a certain trait evolved once is simpler than one in which
that trait evolved twice.
Each additional step on a tree represents an additional "assumption",
and on a simple or "pure" view, the tree that makes the fewest assumptions
should be preferred.

However, this perspective -- which supports the practice of *equal-weights*
parsimony -- treats all assumptions as equivalent; the simplest hypothesis
is the one that makes the fewest assumptions, in this case, assumptions 
of homoplasy.
A more nuanced interpretation of Occam's Razor suggests that the simplest
hypothesis is the one that is least surprising.  A hypothesis predicated
on the existence of a flying spaghetti monster requires only a single
assumption, but we may nevertheless prefer a hypothesis that requires a greater
number of assumptions that are better aligned with what experience has 
previously shown to be true.

This interpretion has two applications for phylogenetics.
The first is that we may wish to prefer trees (which are depictions of 
phylogenetic hypotheses) that concentrate homoplasy in characters that we
believe to be prone to convergent evolution.
This view calls for *character weighting*, that is, assigning less weight
to changes in characters that are believed to be less phylogenetically reliable.
Such characters may be identified by successive approximations [@Farris1969],
by comparing the pattern of their tokens with that of other characters,
by expert judgement, or by other means.

The second application argues that each additional case of homoplasy beyond
the first is successively less surprising: the first homoplasy taught us that
the character was not so reliable, making a second homoplasy less unexpected;
the second observed homoplasy, in turn, makes us less hesitant to accept the
third.  This approach calls for a *step weighting* approach, in which each
additional step in a character receives less penalty than the last.
(Mathematically, this can be expressed in as if it were a character weighting
strategy; but I feel that it has a subtly different motivation and warrants
separate treatment.)

This raises the question of how each steps beyond the first ought to be
penalized.  Ultimately, any concave function (in which each step is penalized
by a positive amount that is smaller than the penalty applied to the previous
step) is consistent with this philosophy [@Arias2004].
The most familiar step weighting approach is Goloboff's [-@Goloboff1993]
implied weighting, where the total cost associated with a character is expressed
as _e_ / (_e_ + _k_), where _e_ is the number of homoplasies within a character,
and _k_ is an arbitrary constant.  As _e_ tends to infinity, this approach tends
to equal weights; as _k_ tends to zero, it tends to clique analysis (in which
a character is either homologous or ignored).
The most appropriate value for _k_ may depend on the number of taxa, the number
and distribution of observed states, and other factors 
(a more detailed treatment -- with references -- will be provided in a revision
of this document).  Moreover, some adjustment must be made for 'missing' data,
i.e. ambiguous tokens, which reduce the opportunity to observe homoplasy
[@Goloboff2014].
Implied weighting is described as an approximation [@Goloboff1993], and I am not
aware of a straightforward interpretation of the 'fit' score, or a principled
definition of the nature of the quantity that is being approximated.

My feeling is that 











This vignette (presently incomplete) will address the interpretation of 
profile scores.

<!--
How are profile scores generated and what do they mean?



In this vignette we'll understand profile scores, and get a small insight 
into how they are calculated in the `TreeSearch` package.

Let's get started by [loading the package](getting-started.html) and
[opening a dataset](https://ms609.github.io/TreeTools/articles/load-data.html). 
We'll work with a dataset generated by Congreve & Lamsdell [-@Congreve2016;-@Congreve2016dd], which contains binary (`0`/`1`) characters,
and with no missing or ambiguous data (`?`s).

```{r Set up, eval=FALSE}
library(TreeSearch)
data(referenceTree)
data(congreveLamsdellMatrices)
dataset <- congreveLamsdellMatrices[[1]]
suppressWarnings(RNGversion("3.5.0")) # Until we can require R3.6.0
set.seed(0)
```
