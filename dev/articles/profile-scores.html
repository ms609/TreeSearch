<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Profile parsimony • TreeSearch</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Profile parsimony">
<meta name="robots" content="noindex">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-V9DV1LS35J"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V9DV1LS35J');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">TreeSearch</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.5.1.9006</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Getting started</h6></li>
    <li><a class="dropdown-item" href="../articles/getting-started.html">Getting started: Installing R and TreeSearch</a></li>
    <li><a class="dropdown-item" href="../articles/tree-search.html">Getting started: Simple tree searches</a></li>
    <li><a class="dropdown-item" href="../articles/tree-space.html">Getting started: Exploring tree space</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced use</h6></li>
    <li><a class="dropdown-item" href="../articles/profile-scores.html">Profile parsimony</a></li>
    <li><a class="dropdown-item" href="../articles/profile.html">Tree search with Profile parsimony</a></li>
    <li><a class="dropdown-item" href="../articles/custom.html">Tree search with custom optimality criteria</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/ms609/TreeSearch/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Profile parsimony</h1>
                        <h4 data-toc-skip class="author">Martin R.
Smith</h4>
            
            <h4 data-toc-skip class="date">2025-04-04</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/ms609/TreeSearch/blob/master/vignettes/profile-scores.Rmd" class="external-link"><code>vignettes/profile-scores.Rmd</code></a></small>
      <div class="d-none name"><code>profile-scores.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="scope-of-this-document">Scope of this document<a class="anchor" aria-label="anchor" href="#scope-of-this-document"></a>
</h2>
<p>This document aims to give a flavour of the justification for using
parsimony in general, and profile parsimony in particular, to
reconstruct evolutionary history; and to summarize the
mathematical/information theoretic underpinning of the profile parsimony
approach.</p>
<p>I’ll open by acknowledging that a philosophical approach is not
everyone’s cup of tea – it’s certainly not my home turf. I tend to
prefer models that can be shown to work well, without worrying too much
about their abstract philosophy – though admittedly, identifying a
‘best’ method is not always straightforward <span class="citation">(e.g.
Smith, 2019)</span>.</p>
<p>I should also acknowledge that practical considerations can influence
the choice of reconstruction method: a tip-dated phylogeny cannot be
accomplished in a parsimony framework, for example; and where a
principled model of evolutionary change is available, as with certain
subsets of molecular data, some see a compelling case for using such
models to infer phylogeny.</p>
<p>My goal here is not to argue that any method is superior, but to
develop a case that profile parsimony rests on a principled underpinning
– and, if it does prove to outperform other methods in certain
circumstances, to give a sense of why this might be.</p>
</div>
<div class="section level2">
<h2 id="a-philosophy-of-parsimony">A philosophy of parsimony<a class="anchor" aria-label="anchor" href="#a-philosophy-of-parsimony"></a>
</h2>
<p>There are a number of complementary perspectives on the philosophical
justification for a parsimony approach <span class="citation">(e.g.
Farris, 1983)</span>, which this brief overview will not do justice to;
but hopefully my unqualified and largely unreferenced perspective
captures some of the nature of the principal arguments. Better-versed
readers are invited to suggest improvements or modifications by e-mail
or by opening a <a href="https://github.com/ms609/TreeSearch/issues/new/" class="external-link">GitHub
issue</a>.</p>
<p>Parsimony has been defended by reference to Occam’s Razor: the
principle that scientists should prefer the simplest explanation that
can provide an adequate account for observed data. In the context of
morphological phylogenetics, “observed data” are codified as observed
character states scored within a matrix. Ideally, a phylogenetic tree
would explain the distribution of character states between taxa by
reconstructing each character state as representing a homologous
feature, with a single evolutionary origin on the tree. An explanation
in which a certain trait evolved once is simpler than one in which that
trait evolved twice; it is less faithful to the information inherent in
the character coding, and attributes less of this information to common
ancestry. Each additional step on a tree can be viewed as an additional
“assumption”, and on a simple or “pure” view, the tree that makes the
fewest assumptions should be preferred.</p>
<p>However, this perspective – which implicitly underpins the practice
of <em>equal-weights</em> parsimony – treats all assumptions as
equivalent; the simplest hypothesis is the one that makes the fewest
assumptions (here, assumptions of homoplasy).</p>
<p>A more nuanced interpretation of Occam’s Razor suggests that the
simplest hypothesis is the one that is least surprising. A hypothesis
predicated on the existence of a flying spaghetti monster may require
only a single (barmy) assumption, but we might nevertheless tend to
prefer a hypothesis that requires a greater number of assumptions that
are better aligned with previous experience.</p>
<div class="section level3">
<h3 id="application-of-occams-razor">Application of Occam’s Razor<a class="anchor" aria-label="anchor" href="#application-of-occams-razor"></a>
</h3>
<p>This interpretation has two applications for phylogenetics. The first
is that we may wish to prefer trees (which are depictions of
phylogenetic hypotheses) that concentrate homoplasy in characters that
we believe to be prone to convergent evolution. This view calls for
<em>character weighting</em>, that is, assigning less weight to changes
in characters that are believed to be less phylogenetically reliable.
Such characters may be identified by successive approximations <span class="citation">(Farris, 1969)</span>, by comparing the pattern of
their tokens with that of other characters, by expert judgement, or by
other <em>a priori</em> means.</p>
<p>The second application argues that each additional case of homoplasy
beyond the first is successively less surprising: the first observation
of homoplasy (rather than some prior intuition) taught us that the
character was not entirely reliable, making a second homoplasy less
unexpected; the second observed homoplasy, in turn, makes us less
surprised by the third. This approach calls for a <em>step
weighting</em> approach, in which each additional step in a given
character receives less penalty than the last. (Mathematically, this can
be expressed in as if it were a character weighting strategy; but I feel
that its <em>a posteriori</em> nature conveys a subtly different
motivation and justifies a separate treatment.)</p>
<p>This raises the question of how each step beyond the first ought to
be penalized. Ultimately, any concave function (in which each step is
penalized by a positive amount that is smaller than the penalty applied
to the previous step) is consistent with this philosophy <span class="citation">(Arias &amp; Miranda-Esquivel, 2004)</span>. The most
widely used step weighting approach is Goloboff’s <span class="citation">(1993)</span> implied weighting, where the total cost
associated with a character is expressed as <em>e</em> / (<em>e</em> +
<em>k</em>), where <em>e</em> is the number of homoplasies within a
character, and <em>k</em> is an arbitrary constant. As <em>e</em> tends
to infinity, this approach tends to equal weights; as <em>k</em> tends
to zero, it tends to clique analysis (in which a character is either
homologous or ignored) <span class="citation">(Farris, 1983)</span>. The
most appropriate value for <em>k</em> may depend on the number of taxa,
the number and distribution of observed states, and other factors <span class="citation">(Goloboff, Carpenter, Arias, &amp; Esquivel, 2008;
Goloboff, Torres, &amp; Arias, 2018)</span> (a more detailed treatment
will be provided in a revision of this document). Moreover, some
adjustment must be made for ‘missing’ data, i.e. ambiguous tokens, which
reduce the opportunity to observe homoplasy <span class="citation">(Goloboff, 2014)</span>. Implied weighting is described
as an approximation <span class="citation">(Goloboff, 1993)</span>, and
I am not aware of a straightforward interpretation of the ‘fit’ score,
or a principled definition of the nature of the quantity that is being
approximated.</p>
</div>
<div class="section level3">
<h3 id="an-information-theoretic-basis">An information theoretic basis<a class="anchor" aria-label="anchor" href="#an-information-theoretic-basis"></a>
</h3>
<p>I argue that the quantity that we should seek to minimise is the
“surprise” of a tree. Information theory is the science of quantifying
unexpectedness. Information is usually measured in <em>bits</em>. One
bit is the amount of information generated by tossing a fair coin: to
record the outcome of a coin toss, I must record either a <code>H</code>
or a <code>T</code>, and with each of the two symbols equally likely,
there is no way to compress the results of multiple tosses.</p>
<p>The Shannon <span class="citation">(1948)</span> information content
of an outcome
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is defined to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(x) = -\log_2{P(x)}</annotation></semantics></math>,
which simplifies to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mi>n</mi></mrow><annotation encoding="application/x-tex">\log_2{n}</annotation></semantics></math>
when all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
outcomes are equally likely. Thus, the outcome of a fair coin toss
delivers
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>2</mn><mo>=</mo><mn>1</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bit</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{2} = 1\textrm{ bit}</annotation></semantics></math>
of information; the outcome of rolling a fair six-sided die contains
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>6</mn><mo>≈</mo><mn>2.58</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{6} \approx 2.58\textrm{ bits}</annotation></semantics></math>
of information; and the outcome of selecting at random one of the 105
unrooted binary six-leaf trees is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>105</mn><mo>≈</mo><mn>6.71</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{105} \approx 6.71\textrm{ bits}</annotation></semantics></math>.</p>
<p>Unlikely outcomes are more surprising, and thus contain more
information than likely outcomes. The information content of rolling a
twelve on two fair six-sided dice is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>1</mn><mn>36</mn></mfrac><mo>≈</mo><mn>5.16</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-\log_2{\frac{1}{36}} \approx 5.16\textrm{ bits}</annotation></semantics></math>,
whereas a seven, which could be produced by six of the 36 possible rolls
(<code>1 &amp; 6</code>, <code>2 &amp; 5</code>, …), is less surprising,
and thus contains less information:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>6</mn><mn>36</mn></mfrac><mo>≈</mo><mn>2.58</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-\log_2{\frac{6}{36}} \approx 2.58\textrm{ bits}</annotation></semantics></math>.
An additional 2.58 bits of information would be required to establish
whether which of the rolls <code>1 &amp; 6</code>,
<code>2 &amp; 5</code>, … occurred.</p>
<p>Now consider two competing explanations for an event: (i), three
consecutive rolls of two dice each produced a seven; (ii), two
consecutive rolls of two dice each produced a twelve. The former event
corresponds to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>2.58</mn><mo>=</mo><mn>7.75</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">3 \times 2.58 = 7.75\textrm{ bits}</annotation></semantics></math>
of information, so is less surprising than the latter, which represents
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>5.16</mn><mo>=</mo><mn>10.34</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">2 \times 5.16 = 10.34\textrm{ bits}</annotation></semantics></math>,
despite involving an additional roll of the dice.</p>
<p>How do we measure the “surprise” associated with additional steps in
a character on a phylogenetic tree? Consider a character with the states
<code>0 0 0 1 1 1</code>. In the most parsimonious situation in which
the character contains a single step on a tree, it is compatible with
nine of the 105 labelled six-leaf trees, and thus represents
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mfrac><mn>9</mn><mn>105</mn></mfrac><mo>=</mo><mn>3.54</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-log_2\frac{9}{105} = 3.54\textrm{ bits}</annotation></semantics></math>
of phylogenetic information. If we are told that the character contains
exactly two steps, it can occur on 63 trees, so yields
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mfrac><mn>63</mn><mn>105</mn></mfrac><mo>=</mo><mn>0.74</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-log_2\frac{63}{105} = 0.74\textrm{ bits}</annotation></semantics></math>.
(The number of trees with <em>m</em> extra steps can be calculated using
theorem 1 of <span class="citation">Carter, Hendy, Penny, Székely, &amp;
Wormald (1990)</span>, implemented in the function <a href="https://ms609.github.io/TreeSearch/reference/Carter1.html"><code>Carter1()</code></a>.)
Learning that a second step occurred meant that 3.54 − 0.74 = 2.81 bits
of information we had previously attributed to common ancestry instead
represent a signature of homoplasy; this quantity measures our degree of
‘surprise’.</p>
<p>If we subsequently learn that the character contains three steps,
then it can occur on any six-leaf tree, and contains no phylogenetic
information. We are less surprised by this third step, which attributes
the remaining 0.74 bits of information to factors other than common
ancestry, because the second step had already reduced the amount of
phylogenetic information we expected the character to hold.</p>
</div>
</div>
<div class="section level2">
<h2 id="profile-parsimony">Profile parsimony<a class="anchor" aria-label="anchor" href="#profile-parsimony"></a>
</h2>
<p>Although this is not how the approach was initially justified <span class="citation">(Faith &amp; Trueman, 2001)</span>, profile parsimony
aims to ascribe as much of the information in each character as possible
to common ancestry; in the example above, it assigns the first extra
step a penalty of 2.81, and the second extra step a penalty of 0.74,
corresponding to the amount of information that could no longer be
assigned to common ancestry after learning of the existence of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>th
step.</p>
<p>Whereas the value of a parsimony score obtained under implied weights
does not have any inherent meaning, the profile parsimony score of a
given tree represents the amount of information within a character
matrix that can be attributed to common ancestry under that tree, which,
if expressed as a proportion of the total cladistic information content
of the characters in the matrix <span class="citation">(Cotton &amp;
Wilkinson, 2008)</span>, gives an indication of the degree of homoplasy
in the underlying character matrix.</p>
<p>Profile parsimony produces a concavity profile that reflects the
opportunity to observe additional steps in each character, unlike
implied weighting, where a single concavity value is applied to all
characters, regardless of the opportunity for additional steps to be
observed.</p>
<p><img src="profile-scores_files/figure-html/plot-concavities-1.png" width="672" style="display: block; margin: auto;"></p>
<p>The graph shows the profiles under implied weighting (with different
concavity constants) and profile parsimony (under different
distributions of the tokens <code>0</code> and <code>1</code> in a
character coded for forty leaves). One prominent difference between the
character of concavity profiles is that implied weighting continues to
assign relatively large penalties to additional steps even when the
distribution of a character is almost random on a tree. Another is that
profile parsimony treats a second (and each subsequent) step as less
surprising if there are fewer opportunities to observe a second step by
chance, on account of there being a smaller number of tokens with the
rarer step; implied weighting is blind to the distribution of tokens
within a character.</p>
</div>
<div class="section level2">
<h2 id="implementation">Implementation<a class="anchor" aria-label="anchor" href="#implementation"></a>
</h2>
<p>The present implementation of profile parsimony in “TreeSearch” is
restricted: inapplicable tokens are treated as ambiguous; partial
ambiguity (e.g. <code>{02}</code>) is treated as complete
(<code>?</code>), and informative states (i.e. states present in more
than one taxon) beyond the first two are ignored (treated as ambiguous).
This reflects the complicated mathematics of calculating the number of
trees with a given number of steps.</p>
<p>Tree length can be calculated with <a href="https://ms609.github.io/TreeSearch/reference/TreeLength.html"><code>TreeLength(concavity = "profile")</code></a>,
and tree search performed with <a href="https://ms609.github.io/TreeSearch/reference/MaximizeParsimony.html"><code>MaximizeParsimony(concavity = "profile")</code></a>.
Data can be prepared for profile parsimony using <a href="https://ms609.github.io/TreeSearch/reference/PrepareDataProfile.html"><code>PrepareDataProfile()</code></a>,
and the profile of a character calculated using <a href="https://ms609.github.io/TreeSearch/reference/StepInformation.html"><code>StepInformation()</code></a>.</p>
</div>
<div class="section level2">
<h2 id="where-next">Where next<a class="anchor" aria-label="anchor" href="#where-next"></a>
</h2>
<ul>
<li>Conduct <a href="profile.html">tree search</a> using profile
parsimony</li>
</ul>
<p>See also:</p>
<ul>
<li><p><a href="getting-started.html">Guide to installation</a></p></li>
<li><p><a href="https://ms609.github.io/TreeSearch/">Documentation
home</a></p></li>
</ul>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-Arias2004" class="csl-entry">
Arias, J. S., &amp; Miranda-Esquivel, D. R. (2004). Profile parsimony
(<span>PP</span>): An analysis under implied weights (<span>IW</span>).
<em>Cladistics</em>, <em>20</em>(1), 56–63. doi: <a href="https://doi.org/10.1111/j.1096-0031.2003.00001.x" class="external-link">10.1111/j.1096-0031.2003.00001.x</a>
</div>
<div id="ref-Carter1990" class="csl-entry">
Carter, M., Hendy, M., Penny, D., Székely, L. A., &amp; Wormald, N. C.
(1990). On the distribution of lengths of evolutionary trees. <em>SIAM
Journal on Discrete Mathematics</em>, <em>3</em>(1), 38–47. doi: <a href="https://doi.org/10.1137/0403005" class="external-link">10.1137/0403005</a>
</div>
<div id="ref-Cotton2008" class="csl-entry">
Cotton, J., &amp; Wilkinson, M. (2008). Quantifying the potential
utility of phylogenetic characters. <em>Taxon</em>, <em>57</em>(1),
131–136.
</div>
<div id="ref-Faith2001" class="csl-entry">
Faith, D. P., &amp; Trueman, J. W. H. (2001). <span class="nocase">Towards an inclusive philosophy for phylogenetic
inference</span>. <em>Systematic Biology</em>, <em>50</em>(3), 331–350.
doi: <a href="https://doi.org/10.1080/10635150118627" class="external-link">10.1080/10635150118627</a>
</div>
<div id="ref-Farris1969" class="csl-entry">
Farris, J. S. (1969). A successive approximations approach to character
weighting. <em>Systematic Biology</em>, <em>18</em>(4), 374–385. doi: <a href="https://doi.org/10.2307/2412182" class="external-link">10.2307/2412182</a>
</div>
<div id="ref-Farris1983" class="csl-entry">
Farris, J. S. (1983). The logical basis of phylogenetic analysis. In N.
Platnick &amp; V. A. Funk (Eds.), <em>Advances in
<span>Cladistics</span>, <span>Vol</span>. 2</em> (pp. 7–36). <span>New
York</span>: <span>Columbia University Press</span>.
</div>
<div id="ref-Goloboff1993" class="csl-entry">
Goloboff, P. A. (1993). Estimating character weights during tree search.
<em>Cladistics</em>, <em>9</em>(1), 83–91. doi: <a href="https://doi.org/10.1111/j.1096-0031.1993.tb00209.x" class="external-link">10.1111/j.1096-0031.1993.tb00209.x</a>
</div>
<div id="ref-Goloboff2014" class="csl-entry">
Goloboff, P. A. (2014). Extended implied weighting. <em>Cladistics</em>,
<em>30</em>(3), 260–272. doi: <a href="https://doi.org/10.1111/cla.12047" class="external-link">10.1111/cla.12047</a>
</div>
<div id="ref-Goloboff2008" class="csl-entry">
Goloboff, P. A., Carpenter, J. M., Arias, J. S., &amp; Esquivel, D. R.
M. (2008). Weighting against homoplasy improves phylogenetic analysis of
morphological data sets. <em>Cladistics</em>, <em>24</em>(5), 758–773.
doi: <a href="https://doi.org/10.1111/j.1096-0031.2008.00209.x" class="external-link">10.1111/j.1096-0031.2008.00209.x</a>
</div>
<div id="ref-Goloboff2018" class="csl-entry">
Goloboff, P. A., Torres, A., &amp; Arias, J. S. (2018). Weighted
parsimony outperforms other methods of phylogenetic inference under
models appropriate for morphology. <em>Cladistics</em>, <em>34</em>(4),
407–437. doi: <a href="https://doi.org/10.1111/cla.12205" class="external-link">10.1111/cla.12205</a>
</div>
<div id="ref-Shannon1948" class="csl-entry">
Shannon, C. E. (1948). A mathematical theory of communication. <em>Bell
System Technical Journal</em>, <em>27</em>, 379–423, 623–656.
</div>
<div id="ref-Smith2019" class="csl-entry">
Smith, M. R. (2019). Bayesian and parsimony approaches reconstruct
informative trees from simulated morphological datasets. <em>Biology
Letters</em>, <em>15</em>(2), 20180632. doi: <a href="https://doi.org/10.1098/rsbl.2018.0632" class="external-link">10.1098/rsbl.2018.0632</a>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://smithlabdurham.github.io/" class="external-link">Martin R. Smith</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
